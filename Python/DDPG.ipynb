{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up working directory\n",
    "import os\n",
    "os.chdir(\"E:/unity/ml-agents/ml-agents-envs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = None  # Name of the Unity environment binary to launch\n",
    "train_mode = True  # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfig, EngineConfigurationChannel\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents_envs:Listening on port 5004. Start training by pressing the Play button in the Unity Editor.\n",
      "INFO:mlagents_envs:Connected new brain:\n",
      "Panda Agent?team=0\n"
     ]
    }
   ],
   "source": [
    "engine_configuration_channel = EngineConfigurationChannel()\n",
    "env = UnityEnvironment(base_port = 5004, file_name=None, side_channels = [engine_configuration_channel])\n",
    "\n",
    "#Reset the environment\n",
    "env.reset()\n",
    "\n",
    "# Set the default brain to work with\n",
    "group_name = env.get_agent_groups()[0]\n",
    "group_spec = env.get_agent_group_spec(group_name)\n",
    "\n",
    "# Set the time scale of the engine\n",
    "engine_configuration_channel.set_configuration_parameters(time_scale = 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Panda Agent?team=0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations :  1\n",
      "Agent state looks like: \n",
      "[[0.829      0.1385     0.         0.         0.5        0.5\n",
      "  1.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]\n",
      " [0.59430003 0.4765     0.         0.         0.5        0.5\n",
      "  1.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]\n",
      " [0.1286     0.26549998 0.         0.         0.5        0.5\n",
      "  1.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]\n",
      " [0.7585     0.1385     0.         0.         0.5        0.5\n",
      "  1.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]\n",
      " [0.45299998 0.704      0.         0.         0.5        0.5\n",
      "  1.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]\n",
      " [0.0513     0.1604     0.         0.         0.5        0.5\n",
      "  1.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]\n",
      " [0.741      0.4765     0.         0.         0.5        0.5\n",
      "  1.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]\n",
      " [0.32275    0.26549998 0.         0.         0.5        0.5\n",
      "  1.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]]\n",
      "Agent state looks like: \n",
      "[0.829  0.1385 0.     0.     0.5    0.5    1.     0.     0.     1.\n",
      " 0.     0.     0.     0.     0.     1.     0.    ]\n",
      "Is there a visual observation ? False\n"
     ]
    }
   ],
   "source": [
    "# Get the state of the agents\n",
    "step_result = env.get_step_result(group_name)\n",
    "\n",
    "# Examine the number of observations per Agent\n",
    "print(\"Number of observations : \", len(group_spec.observation_shapes))\n",
    "\n",
    "# Examine the state space for the first observation for all agents\n",
    "print(\"Agent state looks like: \\n{}\".format(step_result.obs[0]))\n",
    "\n",
    "# Examine the state space for the first observation for the first agent\n",
    "print(\"Agent state looks like: \\n{}\".format(step_result.obs[0][0]))\n",
    "\n",
    "# Is there a visual observation ?\n",
    "vis_obs = any([len(shape) == 3 for shape in group_spec.observation_shapes])\n",
    "print(\"Is there a visual observation ?\", vis_obs)\n",
    "\n",
    "# Examine the visual observations\n",
    "if vis_obs:\n",
    "    vis_obs_index = next(i for i,v in enumerate(group_spec.observation_shapes) if len(v) == 3)\n",
    "    print(\"Agent visual observation look like:\")\n",
    "    obs = step_result.obs[vis_obs_index]\n",
    "    plt.imshow(obs[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import deque, namedtuple\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        #self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "#         self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "#         self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        #self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "#         self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "#         self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if CPU or GPU computation should be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 32        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4       # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0      # L2 weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process for each agent\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        for agent in range(self.num_agents):\n",
    "            self.memory.add(states[agent,:], actions[agent,:], rewards[agent], next_states[agent,:], dones[agent])\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "    def act(self, state, add_noise=True, scale = 2):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        acts = np.zeros((self.num_agents, self.action_size))\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            for agent in range(self.num_agents):\n",
    "                acts[agent,:] = self.actor_local(state[agent,:]).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            acts += scale * self.noise.sample()\n",
    "        return np.clip(acts, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0.0, theta=0.15, sigma=0.15, sigma_min = 0.05, sigma_decay=.975):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_decay = sigma_decay\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \"\"\"Resduce  sigma from initial value to min\"\"\"\n",
    "        self.sigma = max(self.sigma_min, self.sigma*self.sigma_decay)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = group_spec.action_shape\n",
    "state_size = group_spec.observation_shapes[0][0]\n",
    "num_agents = len(step_result.obs[0])\n",
    "\n",
    "episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.5262495077913627\n",
      "1 -0.9462494611507282\n",
      "2 -0.8587494480889291\n",
      "3 -0.5187495144782588\n",
      "4 -0.5737495055655017\n",
      "5 -0.8162494619027711\n",
      "6 -0.6487494906177744\n",
      "7 -1.0037494516000152\n",
      "8 -0.7937494574580342\n",
      "9 -0.5349995022406802\n",
      "10 -0.5687495087040588\n",
      "11 -0.6162494987947866\n",
      "12 -0.7324994656955823\n",
      "13 -1.1049994202330709\n",
      "14 -0.8737494545057416\n",
      "15 -0.5412495018681511\n",
      "16 -1.0274994438514113\n",
      "17 -0.8712494487408549\n",
      "18 -0.3612495008856058\n",
      "19 0.007500411244109273\n",
      "20 -0.6124995091813616\n",
      "21 -1.1049994202330709\n",
      "22 -1.1049994202330709\n",
      "23 -1.1049994202330709\n",
      "24 -0.7587494561448693\n",
      "25 -0.5874994819751009\n",
      "26 -0.698749462608248\n",
      "27 -0.6862494759261608\n",
      "28 -0.5362495047738776\n",
      "29 -0.5849994950694963\n",
      "30 -0.5824995037401095\n",
      "31 -0.5587494954233989\n",
      "32 -0.7587494561448693\n",
      "33 -0.709999484475702\n",
      "34 -0.8237494621425867\n",
      "35 -0.7612494742497802\n",
      "36 -0.34499950846657157\n",
      "37 -1.1049994202330709\n",
      "38 -1.1049994202330709\n",
      "39 -1.1049994202330709\n",
      "40 -1.1049994202330709\n",
      "41 -0.7699994593858719\n",
      "42 -0.7649994806852192\n",
      "43 -0.5549994989996776\n",
      "44 -0.5737494999775663\n",
      "45 -1.028749443590641\n",
      "46 -1.1049994202330709\n",
      "47 -1.1049994202330709\n",
      "48 -1.1049994202330709\n",
      "49 -1.1049994202330709\n",
      "50 -1.1049994202330709\n",
      "51 -1.0599994263611734\n",
      "52 -0.643749481998384\n",
      "53 -1.1049994202330709\n",
      "54 -1.1049994202330709\n",
      "55 -0.7699994593858719\n",
      "56 -0.4899995125597343\n",
      "57 -0.12499956914689392\n",
      "58 -0.25374953844584525\n",
      "59 -0.5324995185947046\n",
      "60 -0.5687494933372363\n",
      "61 -1.0137494439259171\n",
      "62 -1.0487494294065982\n",
      "63 -0.7749994662590325\n",
      "64 -0.5624994928948581\n",
      "65 -0.6949994786409661\n",
      "66 -1.1049994202330709\n",
      "67 -1.1049994202330709\n",
      "68 -0.7399994574952871\n",
      "69 -1.1049994202330709\n",
      "70 -0.7987494673579931\n",
      "71 -0.22999954351689667\n",
      "72 -0.10249957186169922\n",
      "73 -0.387499533186201\n",
      "74 -0.027499574061948806\n",
      "75 -0.6199994985363446\n",
      "76 -0.24874953035032377\n",
      "77 -0.8687494726618752\n",
      "78 -0.06999952998012304\n",
      "79 -0.13124954118393362\n",
      "80 -0.5024995186831802\n",
      "81 -0.489999515353702\n",
      "82 -0.7937494850484654\n",
      "83 -1.1049994202330709\n",
      "84 -0.7562494687736034\n",
      "85 -0.6624994724988937\n",
      "86 -0.7437494611367583\n",
      "87 -1.089999420568347\n",
      "88 -0.7849994720891118\n",
      "89 -0.4399995313724503\n",
      "90 -0.5149995073443279\n",
      "91 0.14250037295278162\n",
      "92 -0.31249953142832965\n",
      "93 -0.3449994991533458\n",
      "94 -0.6024994773324579\n",
      "95 -0.8299994603730738\n",
      "96 -1.0024994472041726\n",
      "97 -0.7212494612904266\n",
      "98 -0.9274994563311338\n",
      "99 -1.1049994202330709\n",
      "100 -0.9912494532763958\n",
      "101 -0.9537494454998523\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    #Reset agent\n",
    "    agent.reset()\n",
    "    env.reset()\n",
    "\n",
    "    #Agent set\n",
    "    agent_scores = np.zeros(num_agents)\n",
    "    step_result = env.get_step_result(group_name)\n",
    "    states = step_result.obs[0]\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        #[Tanh - 1 1]\n",
    "        actions = agent.act(states)\n",
    "        \n",
    "#         if np.random.uniform() < 0.05: \n",
    "#             print(actions)\n",
    "        \n",
    "        # actions\n",
    "        env.set_actions(group_name, actions)\n",
    "        env.step()\n",
    "        step_result = env.get_step_result(group_name)\n",
    "        next_states = step_result.obs[0]\n",
    "\n",
    "        rewards = step_result.reward\n",
    "        dones = step_result.done\n",
    "\n",
    "        #learn\n",
    "        if _ > 10:\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        states = next_states\n",
    "        #print(\"t\",rewards)\n",
    "        agent_scores += rewards[0:num_agents]\n",
    "        \n",
    "        #done\n",
    "        done = step_result.done[0]\n",
    "\n",
    "\n",
    "    print(_,np.mean(agent_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# step_result = env.get_step_result(group_name)\n",
    "# done = False\n",
    "# episode_rewards = 0\n",
    "# while not done:\n",
    "#     action_size = group_spec.action_size\n",
    "#     if group_spec.is_action_continuous():\n",
    "#         action = np.random.randn(step_result.n_agents(), group_spec.action_size)\n",
    "\n",
    "#     if group_spec.is_action_discrete():\n",
    "#         branch_size = group_spec.discrete_action_branches\n",
    "#         action = np.column_stack([np.random.randint(0, branch_size[i], size=(step_result.n_agents())) for i in range(len(branch_size))])\n",
    "#     env.set_actions(group_name, action)\n",
    "#     env.step()\n",
    "#     step_result = env.get_step_result(group_name)\n",
    "#     episode_rewards += step_result.reward[0]\n",
    "#     done = step_result.done[0]\n",
    "# print(\"Total reward this episode: {}\".format(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: 0.785000360570848\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -0.38499961514025927\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: 1.2250001365318894\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.0649994472041726\n",
      "Total reward this episode: -0.4849996091797948\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -0.9449994871392846\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: 0.35500022768974304\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -0.45499951858073473\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: 2.4549999739974737\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -0.6249995091930032\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.0649994472041726\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.0649994472041726\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: 0.7850001975893974\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -0.09499953407794237\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -1.1049994202330709\n",
      "Total reward this episode: -0.9849995011463761\n"
     ]
    }
   ],
   "source": [
    "for episode in range(100):\n",
    "    env.reset()\n",
    "    step_result = env.get_step_result(group_name)\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        action_size = group_spec.action_size\n",
    "        if group_spec.is_action_continuous():\n",
    "            action = np.random.randn(step_result.n_agents(), group_spec.action_size)\n",
    "            \n",
    "        if group_spec.is_action_discrete():\n",
    "            branch_size = group_spec.discrete_action_branches\n",
    "            action = np.column_stack([np.random.randint(0, branch_size[i], size=(step_result.n_agents())) for i in range(len(branch_size))])\n",
    "        env.set_actions(group_name, action)\n",
    "        env.step()\n",
    "        step_result = env.get_step_result(group_name)\n",
    "        episode_rewards += step_result.reward[0]\n",
    "        done = step_result.done[0]\n",
    "    print(\"Total reward this episode: {}\".format(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
